["  import torch\n", "  import torch.nn as nn\n", "+ import time\n", "+ import torch.nn.functional as F\n", "+ from uncertainty_from_motion.src import utils\n", "  \n", "- from .layers import *\n", "+ import sys \n", "+ run_parameters = utils.initiate_parameters(sys.argv[1])\n", "+ class DenseLayer(nn.Sequential):\n", "+     def __init__(self, in_channels, growth_rate):\n", "+         super().__init__()\n", "+         self.add_module('norm', nn.BatchNorm2d(in_channels))\n", "+         self.add_module('relu', nn.ReLU(True))\n", "+         self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size=3,\n", "+                                           stride=1, padding=1, bias=True))\n", "+         self.add_module('drop', nn.Dropout2d(0.2))\n", "  \n", "+     def forward(self, x):\n", "+         x = super().forward(x)\n", "+         x = F.dropout(x, p=run_parameters['p'], training=True)\n", "+         return x\n", "+ \n", "+ \n", "+ class DenseBlock(nn.Module):\n", "+     def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n", "+         super().__init__()\n", "+         self.upsample = upsample\n", "+         self.layers = nn.ModuleList([DenseLayer(\n", "+             in_channels + i*growth_rate, growth_rate)\n", "+             for i in range(n_layers)])\n", "+ \n", "+     def forward(self, x):\n", "+         if self.upsample:\n", "+             new_features = []\n", "+             #we pass all previous activations into each dense layer normally\n", "+             #But we only store each dense layer's output in the new_features array\n", "+             for layer in self.layers:\n", "+                 out = layer(x)\n", "+                 x = torch.cat([x, out], 1)\n", "+                 new_features.append(out)\n", "+             return torch.cat(new_features,1)\n", "+         else:\n", "+             for layer in self.layers:\n", "+                 out = layer(x)\n", "+                 x = torch.cat([x, out], 1) # 1 = channel axis\n", "+             return x\n", "+ \n", "+ \n", "+ class TransitionDown(nn.Sequential):\n", "+     def __init__(self, in_channels):\n", "+         super().__init__()\n", "+         self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n", "+         self.add_module('relu', nn.ReLU(inplace=True))\n", "+         self.add_module('conv', nn.Conv2d(in_channels, in_channels,\n", "+                                           kernel_size=1, stride=1,\n", "+                                           padding=0, bias=True))\n", "+         self.add_module('drop', nn.Dropout2d(0.2))\n", "+         self.add_module('maxpool', nn.MaxPool2d(2))\n", "+ \n", "+     def forward(self, x):\n", "+         x = super().forward(x)\n", "+         x = F.dropout(x, p=run_parameters['p'], training=True)\n", "+         return x\n", "+         # return super().forward(x)\n", "+ \n", "+ \n", "+ class TransitionUp(nn.Module):\n", "+     def __init__(self, in_channels, out_channels):\n", "+         super().__init__()\n", "+         self.convTrans = nn.ConvTranspose2d(\n", "+             in_channels=in_channels, out_channels=out_channels,\n", "+             kernel_size=3, stride=2, padding=0, bias=True)\n", "+ \n", "+     def forward(self, x, skip):\n", "+         out = self.convTrans(x)\n", "+         out = F.dropout(out, p=run_parameters['p'], training=True)\n", "+         out = center_crop(out, skip.size(2), skip.size(3))\n", "+         out = torch.cat([out, skip], 1)\n", "+         return out\n", "+ \n", "+ \n", "+ class Bottleneck(nn.Sequential):\n", "+     def __init__(self, in_channels, growth_rate, n_layers):\n", "+         super().__init__()\n", "+         self.add_module('bottleneck', DenseBlock(\n", "+             in_channels, growth_rate, n_layers, upsample=True))\n", "+ \n", "+     def forward(self, x):\n", "+         # print('bottleneck')\n", "+         # print(x.size())\n", "+         return super().forward(x)\n", "+ \n", "+ \n", "+ def center_crop(layer, max_height, max_width):\n", "+     _, _, h, w = layer.size()\n", "+     xy1 = (w - max_width) // 2\n", "+     xy2 = (h - max_height) // 2\n", "+     return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n", "  \n", "  class FCDenseNet(nn.Module):\n", "      def __init__(self, in_channels=3, down_blocks=(5,5,5,5,5),\n", "                   up_blocks=(5,5,5,5,5), bottleneck_layers=5,\n", "-                  growth_rate=16, out_chans_first_conv=48, n_classes=12):\n", "?                                                                     -\n", "+                  growth_rate=16, out_chans_first_conv=48, n_classes=2):\n", "          super().__init__()\n", "          self.down_blocks = down_blocks\n", "          self.up_blocks = up_blocks\n", "          cur_channels_count = 0\n", "          skip_connection_channel_counts = []\n", "- \n", "          ## First Convolution ##\n", "  \n", "          self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n", "                    out_channels=out_chans_first_conv, kernel_size=3,\n", "                    stride=1, padding=1, bias=True))\n", "          cur_channels_count = out_chans_first_conv\n", "  \n", "          #####################\n", "          # Downsampling path #\n", "          #####################\n", "  \n", "          self.denseBlocksDown = nn.ModuleList([])\n", "          self.transDownBlocks = nn.ModuleList([])\n", "          for i in range(len(down_blocks)):\n", "              self.denseBlocksDown.append(\n", "                  DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n", "              cur_channels_count += (growth_rate*down_blocks[i])\n", "              skip_connection_channel_counts.insert(0,cur_channels_count)\n", "              self.transDownBlocks.append(TransitionDown(cur_channels_count))\n", "  \n", "          #####################\n", "          #     Bottleneck    #\n", "          #####################\n", "  \n", "          self.add_module('bottleneck',Bottleneck(cur_channels_count,\n", "                                       growth_rate, bottleneck_layers))\n", "          prev_block_channels = growth_rate*bottleneck_layers\n", "          cur_channels_count += prev_block_channels\n", "  \n", "          #######################\n", "          #   Upsampling path   #\n", "          #######################\n", "  \n", "          self.transUpBlocks = nn.ModuleList([])\n", "          self.denseBlocksUp = nn.ModuleList([])\n", "          for i in range(len(up_blocks)-1):\n", "              self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n", "              cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n", "  \n", "              self.denseBlocksUp.append(DenseBlock(\n", "                  cur_channels_count, growth_rate, up_blocks[i],\n", "                      upsample=True))\n", "              prev_block_channels = growth_rate*up_blocks[i]\n", "              cur_channels_count += prev_block_channels\n", "  \n", "          ## Final DenseBlock ##\n", "  \n", "          self.transUpBlocks.append(TransitionUp(\n", "              prev_block_channels, prev_block_channels))\n", "          cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n", "  \n", "          self.denseBlocksUp.append(DenseBlock(\n", "              cur_channels_count, growth_rate, up_blocks[-1],\n", "                  upsample=False))\n", "          cur_channels_count += growth_rate*up_blocks[-1]\n", "  \n", "          ## Softmax ##\n", "- \n", "+         if run_parameters['loss'] == 'heteroscedastic':\n", "+             # self.convf = pointwise(32, 2)\n", "+             self.final_conv_pred = nn.Conv2d(in_channels=cur_channels_count,\n", "+                out_channels=1, kernel_size=1, stride=1,\n", "+                    padding=0, bias=True)\n", "+             self.final_conv_logvar = nn.Conv2d(in_channels=cur_channels_count,\n", "+                out_channels=1, kernel_size=1, stride=1,\n", "+                    padding=0, bias=True)\n", "+         else:\n", "-         self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n", "+             self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n", "? ++++\n", "-                out_channels=n_classes, kernel_size=1, stride=1,\n", "?                             ^^^^^^^^^\n", "+                out_channels=1, kernel_size=1, stride=1,\n", "?                             ^\n", "                     padding=0, bias=True)\n", "          self.softmax = nn.LogSoftmax(dim=1)\n", "  \n", "      def forward(self, x):\n", "          out = self.firstconv(x)\n", "- \n", "          skip_connections = []\n", "          for i in range(len(self.down_blocks)):\n", "              out = self.denseBlocksDown[i](out)\n", "              skip_connections.append(out)\n", "              out = self.transDownBlocks[i](out)\n", "- \n", "          out = self.bottleneck(out)\n", "          for i in range(len(self.up_blocks)):\n", "              skip = skip_connections.pop()\n", "              out = self.transUpBlocks[i](out, skip)\n", "              out = self.denseBlocksUp[i](out)\n", "- \n", "+         if run_parameters['loss'] == 'heteroscedastic':\n", "+             x_pred = self.final_conv_pred(out)\n", "+             x_pred = F.relu(x_pred)\n", "+             x_logvar = self.final_conv_logvar(out)\n", "+             out_f = torch.cat((x_pred, x_logvar),1) # merge into one x \n", "+         else:\n", "-         out = self.finalConv(out)\n", "+             out_f = self.finalConv(out)\n", "? ++++           ++\n", "+         return out_f # pred, regularization, logvar\n", "-         out = self.softmax(out)\n", "-         return out\n", "- \n", "  \n", "  def FCDenseNet57(n_classes):\n", "      return FCDenseNet(\n", "          in_channels=3, down_blocks=(4, 4, 4, 4, 4),\n", "          up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n", "          growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n", "  \n", "  \n", "  def FCDenseNet67(n_classes):\n", "      return FCDenseNet(\n", "          in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n", "          up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n", "          growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n", "  \n", "  \n", "- def FCDenseNet103(n_classes):\n", "+ def FCDenseNet103(input_channels=3, n_classes=1, pretrained=True):\n", "      return FCDenseNet(\n", "-         in_channels=3, down_blocks=(4,5,7,10,12),\n", "?                     ^\n", "+         in_channels=input_channels, down_blocks=(4,5,7,10,12),\n", "?                     ^^^^^^^^^^^^^^\n", "          up_blocks=(12,10,7,5,4), bottleneck_layers=15,\n", "-         growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)", "+         growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n", "?                                                                      +\n", "+ \n", "+ def FCDenseNet_make_autoencoder(in_channels=3, pretrained=True):\n", "+     model = FCDenseNet103(input_channels=3, n_classes=1)\n", "+     return model\n"]